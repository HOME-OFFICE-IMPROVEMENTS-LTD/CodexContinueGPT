# ğŸ§  CodexContinueGPT v1 â€“ Architecture Blueprint

## ğŸ§­ Project Vision

CodexContinueGPT is a modular AI assistant framework, acting as a backend brain for developer tools like editors, terminals, SaaS dashboards, and agents. It integrates:

- ğŸ¤– LLMs (OpenAI, Azure, Ollama, TabbyML)
- ğŸ§  Memory (Session + Vector Store)
- ğŸ› ï¸ Plugin System (tools, APIs, agents)
- ğŸ“š Prompt + System Context Libraries
- ğŸ”Œ OpenAPI & API-first interactions

---

## ğŸ§± Key Modules

| Module         | Purpose                                             |
|----------------|-----------------------------------------------------|
| `brain/`       | Memory management (RAM/Vector stores)               |
| `plugins/`     | Tools + APIs (search, shell, docs, GitHub)          |
| `prompts/`     | Prompt engineering + system messages                |
| `router/`      | FastAPI endpoints (LLM/chat/tools/stream)           |
| `frontend/`    | Streamlit dashboard (chat + UI playground)          |
| `runtime/`     | Execution queue, event loop, agent interface        |

---

## ğŸ”„ LLM Agent Flow

1. Receive message input
2. Retrieve memory context
3. Match tools/plugins
4. Construct prompt (system + memory + user)
5. Call LLM via AsyncOpenAI or Ollama
6. Store memory + reply
7. Return assistant response

---

## ğŸ“Š Data Layer

- **SessionMemory:** In-RAM dict (by user/session)
- **Vector Store:** For long-term memory (Future: FAISS, Weaviate)
- **Logging + Tracing:** Persist conversations per agent

---

## ğŸš¦ LLM Providers

| Provider    | SDK           | Notes                             |
|-------------|---------------|-----------------------------------|
| OpenAI      | `openai`      | Already integrated (GPT-3.5/4)    |
| Azure OpenAI| `openai`      | Just add endpoint in `.env`       |
| Ollama      | REST API      | Local fallback support            |
| TabbyML     | REST / socket | Self-hosted autocomplete (TBD)    |

---

## ğŸ“œ Environment Setup (Recap)

- `.env` for LLM keys, models, endpoints
- Devcontainer already supports Python 3.10, FastAPI, Streamlit

---

## ğŸ§© Future Modules

- `codexctl` â€“ CLI assistant with memory
- `github-agent` â€“ PR reviewer bot
- `notebook-agent` â€“ Jupyter-enhanced assistant
- `shell-agent` â€“ Terminal-native Codex bot
- `metrics/` â€“ Logging, analytics, rate-limiting
- `test/` â€“ Unit, integration, memory tests

---

> ğŸ™ï¸ Lead Architect: CodexContinue Assistant v1  
> ğŸ“… Generated: [$(date "+%Y-%m-%d")]  
> ğŸ”’ Anchored for every future step

