# ğŸš€ CodexContinue - AI Dev Assistant Platform Blueprint

## ğŸ¯ Purpose

This repository is for building **CodexContinue**, your own self-hosted AI development assistant platform â€” inspired by **Continue.dev** and **Cursor.com**.

The backend and frontend combined will:

- Offer an **AI-powered in-editor assistant**
- Support **Free / Pro / Business** pricing plans
- Be completely containerized (via Dev Containers)
- Be suitable for **local development** and **cloud deployment** (Azure, AWS, VPS)
- Support multiple LLMs (OpenAI, Azure OpenAI, Ollama, TabbyML, etc.)

---

## ğŸ“‚ Full Project Structure (Detailed)

\`\`\`
CodexContinue/
â”œâ”€â”€ .devcontainer/                     # VSCode Remote Container config
â”‚   â””â”€â”€ devcontainer.json              # - Defines container setup
â”œâ”€â”€ .vscode/                           # VSCode tasks and debugger
â”‚   â”œâ”€â”€ launch.json                    # - Debugger config
â”‚   â””â”€â”€ tasks.json                     # - CLI tasks
â”œâ”€â”€ .github/workflows/                 # CI/CD Workflows (GitHub Actions)
â”‚   â”œâ”€â”€ deploy-backend.yml             # - Deployment workflow (coming soon)
â”‚   â””â”€â”€ test.yml                       # - Run lint/tests on PRs
â”œâ”€â”€ backend/                           # FastAPI Application (LLM Gateway)
â”‚   â”œâ”€â”€ app/                           #   - Core FastAPI source code
â”‚   â”‚   â”œâ”€â”€ main.py                    #     - App root & router includes
â”‚   â”‚   â”œâ”€â”€ routes/                    #     - API endpoints (chat, auth...)
â”‚   â”‚   â”œâ”€â”€ services/                  #     - GPT, LLM service wrappers
â”‚   â”‚   â””â”€â”€ utils/                     #     - Helper functions
â”‚   â”œâ”€â”€ requirements.txt               #   - Python dependencies
â”‚   â”œâ”€â”€ Dockerfile                     #   - Optional containerization
â”‚   â””â”€â”€ tests/                         #   - Unit & integration tests
â”œâ”€â”€ frontend/                          # Frontend Web Interface (TBD)
â”‚   â””â”€â”€ README.md                      # Placeholder (React, Streamlit, etc.)
â”œâ”€â”€ docs/                              # Internal docs & business strategy
â”‚   â”œâ”€â”€ SaaS_Plans.md                  # Pricing model: Free, Pro, Business
â”‚   â””â”€â”€ Setup_Guide.md                 # Dev onboarding / architecture
â”œâ”€â”€ README.md                          # General readme
â””â”€â”€ PROJECT_BLUEPRINT.md               # ğŸ”¥ Permanent memory anchor (this file)
\`\`\`

---

## ğŸŒ Deployment Options

- Azure App Service (Python or Docker)
- AWS EC2 / Lightsail
- Fly.io / Railway.app / Render
- Docker Compose for local testing
- GitHub Codespaces or Dev Container

---

## ğŸ’³ SaaS Tiers (From \`docs/SaaS_Plans.md\`)

| Plan     | Description                         | Features                                 |
| -------- | ----------------------------------- | ---------------------------------------- |
| Free     | Limited usage + OpenAI keys only    | Chat, Autocomplete, Docs                 |
| Pro      | Unlimited LLM access + local models | All features + local Ollama, TabbyML     |
| Business | Team support, APIs, telemetry       | Org dashboards, Azure OpenAI, API tokens |

---

## ğŸ”¥ Status Tracker

- Milestone achieved: Conversational shell agents supported as of 2025-05-03.

- [x] Devcontainer Setup
- [x] Backend running with FastAPI
- [ ] Frontend UI starter (TBD)
- [ ] CI/CD pipelines (GitHub Actions)
- [ ] Multi-LLM integration (Azure OpenAI, Ollama)
- [ ] Billing Integration (Stripe/Fake API)
- [ ] Full SaaS Launch

---

## ğŸ’¡ Long-Term Goals

- Real-time chat UI (Next.js or Streamlit)
- Workspace memory + chat history per user
- Multi-model routing (e.g., GPT-4, Mistral, Claude)
- Local LLM fallback (Ollama, TabbyML)
- VSCode Extension integration
- Full telemetry, analytics, and monitoring

---

## ğŸ§  Developer Notes

- This blueprint is **always up to date** with your real intent
- It is the single source of truth for current progress
- Any LLM reading this file will instantly know your project direction
- All copilots (GitHub Copilot, Cursor, TabbyML, Ollama) will **read this first** when helping.

---

> ğŸ›¡ï¸ Saved by Captain Mo + Copilot. Updated: 2025

ğŸ§  Milestone â€” 2025-05-04
- Real-time `/chat` plugin dispatch enabled via PluginAgent.
- Chat endpoint parses `/run <plugin> <input>` before calling LLM.
- Memory logging and plugin middleware are now unified.